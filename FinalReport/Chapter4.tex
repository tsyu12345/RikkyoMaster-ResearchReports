\chapter{実験結果と考察}
本章では,前章で述べた提案手法の有効性を検証するために行ったシミュレーション実験の結果について報告する.
また,各実験結果に基づき,提案手法が持つ課題やその改善の可能性について考察する.

\section{避難者探索タスク実験}
\subsection{モデル学習結果}
図\ref{fig:YokosukaModel-Search-Result}～図\ref{fig:NumazuModel-Search-Result2}は本実験における各都市環境のマルチエージェントモデルの学習過程のグラフである.エントロピーの推移,グループ報酬の推移,ポリシー関数の平均損失
,価値関数の平均損失のグラフを示す.
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/YokosukaSearch-ModelResult1.png}
      \caption{横須賀市環境での訓練結果}
      \label{fig:YokosukaModel-Search-Result}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/NumazuSearch-ModelResult1.png}
      \caption{沼津市環境での訓練結果}
      \label{fig:NumazuModel-Search-Result}
  \end{subfigure}
  \caption{両都市モデルでのエントロピー(上)とグループ報酬(下)の推移}
  \label{fig:SearchModel-Result1}
\end{figure}
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/YokosukaSearch-ModelResult2.png}
      \caption{横須賀市環境での訓練結果}
      \label{fig:YokosukaModel-Search-Result2}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/NumazuSearch-ModlRsult2.png}
      \caption{沼津市環境での訓練結果}
      \label{fig:NumazuModel-Search-Result2}
  \end{subfigure}
  \caption{ポリシー関数の平均損失(左)と価値関数の平均損失(右)}
  \label{fig:SearchModel-Result-Errors}
\end{figure}
まず,図\ref{fig:YokosukaModel-Search-Result}と図\ref{fig:NumazuModel-Search-Result}のエントロピーの結果であるが,これはモデルの行動決定のランダムさを示す指標であり,この値が低い程行動出力がランダムでない,すなわちモデルの学習した方策に基づいて行動を出力していると見なすことができる.
しかし,両方の都市において,この値が学習の経過とともに上昇していることが確認でき,その値は学習終了時までに横須賀市のケースでは1.5から3.0程度,沼津市のケースでは1.5から4.5程度と高い値を示した.
また,グループ報酬の推移であるが,両都市ともに学習が進むにつれ減少傾向にあり,訓練課程での報酬の最大化が達成されていないことがわかる.横須賀市のケースでは学習後半から若干ではるが獲得グループ報酬が微増してきている.
一方,沼津市のケースでは,学習終了時までに報酬が減少し続けていることがわかる.訓練開始から終了までのグループ報酬の推移は,横須賀市のケースでは0.3から0.22程度,沼津市のケースでは0.87から0.5程度に落ち着いた.\par 
次に図\ref{fig:YokosukaModel-Search-Result2}と図\ref{fig:NumazuModel-Search-Result2}のポリシーと価値関数の平均損失のグラフを見る.(左)のポリシー関数の平均損失のグラフは,エージェントの方策がどの程度変化しているかを示すグラフで学習が成功するにつれ,減少することが期待されるグラフである.
両都市ともに,安定して減少しているとは言い難く,学習終了時までに収束していないことが確認できる.ただし,沼津市のケースでは若干ではあるが,学習終了時までに振れ幅はあるものの損失が減少傾向にあることが読み取れる.
訓練開始から終了までのポリシー関数の平均損失の値は,横須賀市のケースでは0.3から0.25まで減少した後に0.27まで損失が増加した.沼津市のケースでは,0.32から0.05まで減少した.
最後に,(右)価値関数の平均損失のグラフであるが,これはモデルの予測精度を示すグラフであり,エージェントの学習中は増加し,累積報酬が安定すると減少するグラフである.
これを読み解くと,両都市ともに学習初期段階は増加傾向にあるが,学習の進行につれ減少傾向にあるのが読み取れる.
訓練開始から終了までの価値関数の平均損失の値は,横須賀市のケースでは0.18から始まり0.38程度まで上昇した後,最終的には0.08まで減少した.
沼津市のケースも同様の値の範囲で減少傾向を示した.
価値関数の損失が減少しているにもかかわらず,累積報酬が減少している場合,エージェントの行動方針が誤った方向に収束している可能性がある.

\subsection{実験結果}
本節では,避難者探索タスクの実験結果を報告する.
本実験では,マルチエージェントモデルとの比較実験と合わせて以下の2パターンにおける最終的な避難者探索完了率の推移を元に,
訓練したマルチエージェントモデルモデルの有効性を評価する.
なお,シミュレーションの制限時間は,各都市ごとに異なるが,100秒毎に段階的に増やしていく形で設定し,記録した.
\begin{enumerate}
  \item ルールベースでの探索
  \item 学習済みマルチエージェントモデルによる探索
\end{enumerate}

\subsubsection{横須賀市のケース}
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.51\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/YokosukaSearch-RuleResult.png}
      \caption{ルールベースでの探索}
      \label{fig:YokosukaSearch-RuleResult}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/YokosukaSearch-AgentsResult.png}
      \caption{学習済みマルチエージェントモデルによる探索}
      \label{fig:YokosukaSearch-AgentResult}
  \end{subfigure}
  \caption{横須賀市のケースにおける避難者探索完了率の推移}
  \label{fig:YokosukaSearch-Result}
\end{figure}
横須賀市のケースでは,全体的にマルチエージェントモデルによる探索よりも,各エージェントがルールベースでランダムに探索する方法の方がエピソード終了までに高い発見率を示した.
ルールベースでの探索の方が最終的な発見率が33\%から55\%程度なのに対し,マルチエージェントモデルによる探索はほとんどのエピドートにおいて,20\%から最大でも30\%程度の発見率に留まった.
経過時間当たりの発見率の上昇度合も,ルールベースでの探索の方が高い値を示しており,マルチエージェントモデルによる探索は,エピソード終了までに十分な数の避難者を発見できていないことがわかる.

\subsubsection{沼津市のケース}
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/NumazuSearch-RuleResult.png}
      \caption{ルールベースでの探索}
      \label{fig:NumazuSearch-RuleResult}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/NumazuSearch-AgentsResult.png}
      \caption{学習済みマルチエージェントモデルによる探索}
      \label{fig:NumazuModel-Result2}
  \end{subfigure}
  \caption{沼津市のケースにおける避難者探索完了率の推移}
  \label{fig:Model-Result-Errors}
\end{figure}
沼津市のケースでも横須賀市のケースと同様に,マルチエージェントモデルによる探索よりも,各エージェントがルールベースでランダムに探索する方法の方がエピソード終了までに高い発見率を示した.
ルールベースでの探索が,多くのエピソードにおいて70\%から85\%程度の発見率を示しているのに対し,マルチエージェントモデルによる探索は,40\%程度の発見率に留まった.
ルールベースのみの結果に着目すると,横須賀市のケースよりも非常に高い発見率を示しており,エピソード終了までに最大で90\%程度の避難者を発見できているケースがあるのが分かる.

\subsection{結果の考察}
実験では,マルチエージェント強化学習モデルとルールベースモデルの2つの行動パターンを比較し,避難者発見率およびその推移を評価した.

避難者探索のタスクにおいては,両都市ともにマルチエージェントモデルによる探索の方が,ルールベースでの探索に比べて避難者の発見率が低い結果となった.
まず,結果として明らかになったのは,ルールベースモデルがマルチエージェントモデルよりも高い避難者発見率を達成した点である.
図\ref{fig:YokosukaSearch-RuleResult}および図\ref{fig:NumazuSearch-RuleResult}から,ルールベースモデルは横須賀市および沼津市の両環境で,最終的に50\%から90\%の発見率を示しており,エピソード中の発見率の増加傾向も顕著であった.
一方,マルチエージェントモデルでは,多くのエピソードで発見率が20\%から40\%程度に留まり,ルールベースモデルに劣る結果となった.
ただし,横須賀市におけるケースでは図\ref{fig:YokosukaSearch-RuleResult}および図\ref{fig:YokosukaSearch-AgentResult}より,今回のマルチエージェントモデルが優れた点として最終的な発見率のバラつきがルールベースの場合よりも小さい.
ルールベースでは,今回のシミュレーション回数7回のうち,発見率が30\%~60\%程度と,約30\%程度の開きがある.
一方マルチエージェントモデルの場合は,発見率が12\%～30\%程度とその開きは18\%程度となっており,結果としての精度としては低いが,ルールベースよりも安定した結果を期待できる可能性がある.


この結果の原因として考えられる要因は以下の通りである.まず,マルチエージェントモデルの学習過程において,エントロピーが学習の進行とともに上昇している点が挙げられる（図\ref{fig:YokosukaModel-Search-Result}および図\ref{fig:NumazuModel-Search-Result}参照）.エントロピーはエージェントの行動選択のランダムさを示す指標であり,その値が高い場合,行動方針が収束せず,適切な探索戦略が確立できていないことを示している.これに加え,報酬設計において,個別報酬およびグループ報酬が避難者発見数に直接比例しているため,エージェントが短期的な目標（局所的な避難者発見）に偏り,グローバルな探索戦略を学習できていない可能性がある.

次に,ポリシー損失および価値関数損失の推移（図\ref{fig:YokosukaModel-Search-Result2}および図\ref{fig:NumazuModel-Search-Result2}参照）を見ると,特にポリシー損失が学習終了時まで安定して収束していない点が観察された.
これは,エージェントが適切な方策を十分に学習できておらず,環境内で効果的な行動を選択できないことを示している.一方,価値関数損失は学習が進むにつれ減少しており,エージェントが環境内の状態価値をある程度正確に予測していることが分かる.
しかし,この予測精度の向上が報酬の最大化には結びついていないことから,モデルの報酬設計にさらなる調整が必要である.

ルールベースモデルがマルチエージェントモデルを上回った理由として,環境特性の影響も考慮する必要がある.
今回の環境では避難者の出現位置は,各都市の道路状にランダム出現するという条件であった.
そのため避難者の出現ポイントが環境内で均一である可能性が高く,ランダムに移動するルールベースモデルが高い発見率を達成しやすい条件となっていた可能性がある.
一方で,マルチエージェントモデルは探索戦略の収束が不十分であったため,これらの単純な条件に適応しきれなかったと考えられる.
ここ部分に関しては,住宅街等の狭い道路より,国道等の大通りの方が路上に存在する避難者の割合は高いはずであるため,シミュレーション条件の再考が必要である可能性がある.

今後の改善点としては,まず報酬設計の見直しが必要である.
例えば,エージェントが効率的な探索を行えるよう,未探索エリアの探索や他エージェントとの協調行動を促進する報酬を追加することが考えられる.
また,エントロピーの上昇を抑えつつ探索と収束のバランスを取るために,ハイパーパラメータの調整や方策正則化を導入することが有効と考えられる.

以上のように,本研究における避難者探索タスクの結果から,マルチエージェントモデルの有効性を最大化するためには報酬設計や学習設定のさらなる最適化が求められることが示唆された.

\section{避難所誘導タスクの結果}
\subsection{モデル学習結果}
図\ref{fig:YokosukaModel-Result}～図\ref{fig:NumazuModel-Result2}は,本実験における各都市環境のマルチエージェントモデルの学習過程のグラフである.エントロピーの推移,グループ報酬の推移,ポリシー関数の平均損失
,価値関数の平均損失のグラフを示す.
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/YokosukaModel-Result.png}
      \caption{横須賀市環境での訓練結果}
      \label{fig:YokosukaModel-Result}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/NumazuModel-Result.png}
      \caption{沼津市環境での訓練結果}
      \label{fig:NumazuModel-Result}
  \end{subfigure}
  \caption{両都市モデルでのエントロピー(上)とグループ報酬(下)の推移}
  \label{fig:Model-Result1}
\end{figure}
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Yokosuka-Loss.png}
      \caption{横須賀市環境での訓練結果}
      \label{fig:YokosukaModel-Result2}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Numazu-Loss.png}
      \caption{沼津市環境での訓練結果}
      \label{fig:NumazuModel-Result2}
  \end{subfigure}
  \caption{ポリシー関数の平均損失(左)と価値関数の平均損失(右)}
  \label{fig:Model-Result-Errors}
\end{figure}
どちらの都市においてもエントロピーが学習が進むにつれ減少しており,学習によりモデルの行動が収束していることがわかる.
最終的な数値の大小としては,横須賀市での学習環境の方が沼津市に比べてエントロピーが低く,モデルの行動出力としては前者の環境の方が安定性があると言える.\par 
次にグループ報酬の推移について見ると,横須賀市の環境においては,エピソードの進行に伴いグループ報酬が減少してしまっている.
対して,沼津市の環境においては,横須賀市の環境よりもバラつきはあるものの,全体としては学習が進むにつれて微増しており,エントロピーの結果とも合わせると良い方向に,グループ報酬の最大化にむけて方策が収束していったことが分かる.\par
しかし,最終的なグループ報酬の値に着目すると,両方の環境において0.3から0.45程度の報酬しか得ておらず,訓練全体を通してあまり良い結果が得られていないことがわかる.\par
また,モデルの予測精度を示す価値関数の平均損失のグラフの値が,横須賀市の場合は1000,沼津市の場合は5000程度と高い値を示していることがわかる.
価値関数の損失の推移は,一般に報酬が安定すると減少するが,横須賀市の場合はほぼ横這いとなっており,図\ref{fig:YokosukaModel-Result}のグループ報酬の推移と合わせると,モデルの学習が十分に進んでいないことがわかる.
沼津市の場合は,価値関数の損失は学習の経過と共に減少しているように見えるが,その値は高い状態が続いており,モデルの予測精度についても十分な精度が得られていないことがわかる.\par
\subsection{実験結果}
本節では,避難所誘導タスクの実験結果を報告する.本実験では,マルチエージェントモデルとの比較実験と合わせて以下の3パターンにおける最終的な避難完了率や時間経過ごとの避難完了率の推移を元に,
訓練したマルチエージェントモデルモデルの有効性を評価する.
なお,シミュレーションの制限時間は,各都市ごとに異なるが,100秒毎に段階的に増やしていく形で設定し,記録した.
\begin{enumerate}[(a)]
  \item 避難者のみで避難行動を行う場合
  \item ルールベースで行動するエージェントモデルによる誘導
  \item 学習済みマルチエージェントモデルによる誘導
\end{enumerate}
\subsubsection{横須賀市のケース}
シミュレーション制限時間は1800秒から2400秒の間で,エピソード毎に100秒ずつ増加する形で検証した.
以下に,横須賀市のケースにおいて,各ケース毎に複数回シミュレートした結果の経過時間ごとの避難完了率の推移を示す.横軸がシミュレーション経過時間(秒)であり,縦軸が避難完了率である.
\begin{figure}[H]
  \centering
  % 上段左側
  \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Yokosuka-EvaOnly-ERE.png} % 画像A
      \caption{(a).避難者のみで避難行動を行う場合}
      \label{fig:yokosuka-guid-graph-a}
  \end{minipage}
  \hfill % 隙間を調整
  % 上段右側
  \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Yokosuka-RuleModel-ERE.png} % 画像B
      \caption{(b).ルールベースでの誘導の場合}
      \label{fig:yokosuka-guidgraph-b}
  \end{minipage}
  
  % 改行して下段中央
  \vspace{1em} % 適宜調整
  \begin{minipage}{0.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Yokosuka-AgentModel-ERE.png} % 画像C
      \caption{(c).学習済みマルチエージェントモデルによる誘導の場合}
      \label{fig:yokosuka-guidgraph-c}
  \end{minipage}
\end{figure}
横須賀市における環境においては,(c)マルチエージェントモデルによる誘導よりも,(b)ルールベースまたは(a)避難者のみで避難行動を行う場合の方が避難完了率の変化が速く,多くのエピソードにおいて最終的な避難完了率が$90\%$を超えており,高いことがわかる.
マルチエージェントモデルによる誘導では,多くの場合で避難完了率が60\%から70\%程度で収束しており,多くのケースで避難者全員を制限時間以内に避難所まで誘導することを達成出来なかった.
また,ルールベースと避難者のみでの結果のグラフに着目すると,避難者のみで行動する場合は,避難率が100\%になるまでに要した時間が1500秒前後なのに対し,ルールベースでの誘導を導入した場合は1250秒前後と,出現した避難者全員が避難完了するまでの時間が数分程度短縮されていることがわかる.\par

\subsubsection{沼津市のケース}
シミュレーション制限時間は240秒から1800秒の間で,エピソード毎に100秒ずつ増加する形で検証した.
以下に,沼津市のケースにおいて,各ケース毎に複数回シミュレートした結果の経過時間ごとの避難完了率の推移を示す.横軸がシミュレーション経過時間(秒)であり,縦軸が避難完了率である.
\begin{figure}[H]
  \centering
  % 上段左側
  \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Numazu-EvaOnly-ERE.png} % 画像A
      \caption{(a).避難者のみで避難行動を行う場合}
      \label{fig:numazu-guid-graph-a}
  \end{minipage}
  \hfill % 隙間を調整
  % 上段右側
  \begin{minipage}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Numazu-RuleModel-ERE.png} % 画像B
      \caption{(b).ルールベースでの誘導の場合}
      \label{fig:numazu-guid-graph-b}
  \end{minipage}
  
  % 改行して下段中央
  \vspace{1em} % 適宜調整
  \begin{minipage}{0.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Numazu-AgentModel-ERE.png} % 画像C
      \caption{(c).学習済みマルチエージェントモデルによる誘導の場合}
      \label{fig:numazu-guid-graph-c}
  \end{minipage}
\end{figure}
沼津市のケースでは,(a)避難者のみで避難行動を行う場合と(b)ルールベースでの誘導を導入時の避難完了率の推移に大きな差は見られず,
どちらのケースも多くのエピソードで,短時間で避難完了率が90\%以上に達しており,全体的に迅速な避難が実現されている.
避難者のみで行動を行う場合,避難完了率が90\%を超えるまで開始から250秒以上経過してからが多いが,ルールベースでの誘導時は若干ではあるが250秒以下で避難完了率90\%を達成しているケースが多く,前者よりも避難完了率の上昇が早いことがわかる.
一方で,学習済みマルチエージェントモデルによる誘導時の避難完了率の推移を見ると,避難者のみで避難行動を行う場合やルールベースでの誘導を導入時と比較して,避難完了率の上昇が遅いことがわかる.
制限時間が短いエピソードでは避難完了率が20\%から40\%程に留まっている他,避難完了率が90\%を超えるまでに1200秒以上必要なエピソードが多く見られ,他のケースと比較して遅い避難完了率の推移が見られる.

\subsection{結果の考察}
避難所誘導タスクにおいて,図\ref{fig:yokosuka-guidgraph-c}から図\ref{fig:numazu-guid-graph-c}の結果から今回訓練したマルチエージェントモデルでは,多くの場合において避難完了率が90\%を超えることができず,避難者全員を制限時間内に避難所まで誘導することが難しいことがわかった.
また,経過時間あたりの避難完了率の伸び率からも,ルールベースでの誘導や避難者のみでの避難行動を行う場合と比較して,今回作成したマルチエージェントモデルによる誘導の方が遅いことが示された.
これは,図\ref{fig:Model-Result1}の学習結果からもわかるように,モデルの学習が不十分であることが原因であると考えられる.
訓練全体を通じて,高い避難完了率を達成できるような方策をエージェントが経験できず,誘導人数や現在位置,避難所の収容人数に基づいた適切な誘導先の避難所を選択できなかったと考えられる.
また,モデルの予測精度を評価する指標の価値関数の平均損失の値が非常に高いことから,モデルの予測精度も低いことが言え,エージェントが報酬を最大化する適切な行動を出力できないということが言えるだろう.加えて,損失関数の減少が収束していないことからも,モデルの学習が不十分であると思われる.
これらは,訓練時間の不足や,ニューラルネットワークのハイパーパラメータの調整にまだ改善の余地があることを示しており,今後の課題として挙げられる.

また,エージェントの行動過程を観察分析すると,近隣の受け入れ可能な避難所を選択するのではなく,遠方の収容人数が多い避難所を選択する傾向が多く見られた.
このことが,他の方法と比較して遅い避難完了率の推移に繋がったと考えられる.
また,図\ref{fig:GuidAgent-Tink1}のように,全エージェントが１箇所の避難所に殺到してしまうケースも散見された.
\begin{figure}[H] 
  \centering 
  \includegraphics[width=0.75\textwidth]{Figures/GuidAgent-Tink1.png}
  \caption{1箇所の避難所に殺到するエージェントの行動過程}
  \label{fig:GuidAgent-Tink1}
\end{figure}
この原因としては,エージェントの行動決定時に,避難所と自身の距離に応じた報酬を与えていないことが原因で,エージェントが距離に応じた適切な避難所を選択できなかった可能性が考えられる.
また,報酬の付与のタイミングも問題であった可能性がある.今回グループ報酬はエピソード終了時の全体の避難完了率を報酬としていた.また個別報酬は,各エージェントの避難所選択時ではなく,避難者が避難所に到達した際に逐次報酬を与えていた.
このため,エージェントは避難所に到達するまでの行動に対して報酬を受け取ることができず,適切な避難所を選択するための情報が不足していた可能性がある.

沼津市の方が横須賀市よりも全体的な避難完了率が高いこと,経過推移が早いことが図\ref{fig:yokosuka-guid-graph-a}から図\ref{fig:numazu-guid-graph-c}の結果からわかる.
これは,沼津市の避難所配置が図\ref{fig:NumazuMap}のように\ref{fig:yokosukaMap}横須賀市よりも個数が多い他,対象範囲内にあまり位置に偏りなく避難所が配置されていることが影響していると考えられる.
横須賀市の場合は,対象範囲の両端それぞれに２か所の避難所が設けられているが,沼津市の場合は,横須賀市ほど避難所の配置は偏っておらず,その配置は放射状に配置されている他,近距離に他の避難所があるといった特性がある.
この様な都市ごとの環境的要因から1棟あたりの収容人数が横須賀市よりも少なく設定されているのにも関わらず,避難者が直ぐに近隣の避難所までたどり着けた傾向があり,全体的な避難完了率が横須賀市よりも高い結果となったと考えられる.
\begin{figure}[H]
  \centering
  % 1枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.85\textwidth]{Figures/Yokosuka-Map.png}
      \caption{横須賀市環境の全体図}
      \label{fig:yokosukaMap}
  \end{subfigure}
  % 2枚目の画像
  \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=\textwidth]{Figures/Numazu-Map.png}
      \caption{沼津市環境の全体図}
      \label{fig:NumazuMap}
  \end{subfigure}
  \caption{各都市の全体図(緑:避難所, 灰色:道路)}
  \label{fig:Maps}
\end{figure}